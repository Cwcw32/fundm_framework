{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 首先对hook的应用场景做一下总结\n",
    "①在自己构建模型的时候对某一层的输入进行相应的变换\n",
    "②别人已经训练好的模型，希望能够得到某一层的输出"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "outputs": [],
   "source": [
    "# 以BERT为例子\n",
    "import torch"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "outputs": [],
   "source": [
    "from torch import nn, Tensor\n",
    "\n",
    "class VerboseExecution(nn.Module):\n",
    "    def __init__(self, model: nn.Module):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "\n",
    "        # 模型的每一层都注册这个hook\n",
    "        for name, layer in self.model.named_children():\n",
    "            layer.__name__ = name\n",
    "            #显然，这个表达式的意思是得到对应层的名字和输出向量的维度大小\n",
    "            layer.register_forward_hook(\n",
    "                lambda layer,_,__: print(f\"{layer.__name__}\")\n",
    "            )\n",
    "\n",
    "    def forward(self, x,y,z,step):\n",
    "        return self.model(x,y,z,step)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer,BertModel"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "outputs": [],
   "source": [
    "class BMRC(nn.Module):\n",
    "    \"\"\"\n",
    "\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 args):\n",
    "        hidden_size = args.hidden_size\n",
    "\n",
    "        super(BMRC, self).__init__()\n",
    "\n",
    "        if args.bert_model_type.find('bert-base-uncased')!=-1:# 只是使用BERT模型\n",
    "            self._bert = BertModel.from_pretrained('../../bert/bert-base-uncased',output_attentions=True)\n",
    "            self._tokenizer = BertTokenizer.from_pretrained('../../bert/bert-base-uncased')\n",
    "            print('Bertbase model loaded')\n",
    "        else:\n",
    "            raise KeyError('Config.args.bert_model_type should be bert-based-uncased. ')\n",
    "\n",
    "        self.classifier_start = nn.Linear(hidden_size, 2)\n",
    "\n",
    "        self.classifier_end = nn.Linear(hidden_size, 2)\n",
    "\n",
    "        self._classifier_sentiment = nn.Linear(hidden_size, 3)\n",
    "\n",
    "        for name, layer in self._bert.named_children():\n",
    "            layer.__name__ = name\n",
    "            #显然，这个表达式的意思是得到对应层的名字和输出向量的维度大小\n",
    "            layer.register_forward_hook(\n",
    "                lambda layer,_,__: print(f\"{layer.__name__}\")\n",
    "            )\n",
    "\n",
    "    def forward(self,\n",
    "                query_tensor,\n",
    "                query_mask,\n",
    "                query_seg,\n",
    "                step):\n",
    "        \"\"\"\n",
    "\n",
    "        :param query_tensor:就输入BERT的就行，本任务来讲是[CLS]Question（根据情况要更改）[SEP]原句子[PADDING]...[PADDING]\n",
    "        :param query_mask:\n",
    "        :param query_seg:\n",
    "        :param step: 用来区分是span提取还是情感分类\n",
    "        :return:\n",
    "        \"\"\"\n",
    "\n",
    "        hidden_states = self._bert(query_tensor, attention_mask=query_mask, token_type_ids=query_seg)\n",
    "        att=hidden_states.attentions[0][0][0]\n",
    "        hidden_states=hidden_states.last_hidden_state\n",
    "        if step == 0:  # 预测实体（即aspect或opinion）\n",
    "            out_scores_start = self.classifier_start(hidden_states)\n",
    "            out_scores_end = self.classifier_end(hidden_states)\n",
    "            return out_scores_start, out_scores_end,att\n",
    "        else:  # 预测情感（即sentiment）\n",
    "            cls_hidden_states = hidden_states[:, 0, :]\n",
    "            cls_hidden_scores = self._classifier_sentiment(cls_hidden_states)\n",
    "            return cls_hidden_scores"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at ../../bert/bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bertbase model loaded\n"
     ]
    }
   ],
   "source": [
    "class OPTION():\n",
    "    def __init__(self):\n",
    "        self.bert_model_type='bert-base-uncased'\n",
    "        self.acc_batch_size=1\n",
    "        self.cuda=True\n",
    "        self.work_nums=1\n",
    "        self.model_name='ROBMRC'\n",
    "        self.hidden_size=768\n",
    "\n",
    "opt=OPTION()\n",
    "model=BMRC(opt)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "outputs": [],
   "source": [
    "verbose_resnet = VerboseExecution(model)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "outputs": [],
   "source": [
    "tokenizer=AutoTokenizer.from_pretrained('../../bert/bert-base-uncased')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "outputs": [],
   "source": [
    "dummy_input = 'hello world'\n",
    "S_A_tokenized=tokenizer(dummy_input)\n",
    "_forward_S_A_query,_forward_S_A_query_mask,_forward_S_A_query_seg=S_A_tokenized['input_ids'],S_A_tokenized['attention_mask'],S_A_tokenized['token_type_ids']\n",
    "_forward_S_A_query_mask=Tensor(_forward_S_A_query_mask).long()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "outputs": [],
   "source": [
    "_forward_S_A_query=Tensor(_forward_S_A_query).long()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "outputs": [],
   "source": [
    "_forward_S_A_query_seg=Tensor(_forward_S_A_query_seg).long()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "outputs": [],
   "source": [
    "_forward_S_A_query_seg=_forward_S_A_query_seg.unsqueeze(dim=0)\n",
    "_forward_S_A_query=_forward_S_A_query.unsqueeze(dim=0)\n",
    "_forward_S_A_query_mask=_forward_S_A_query_mask.unsqueeze(dim=0)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embeddings\n",
      "encoder\n",
      "pooler\n",
      "_bert\n",
      "classifier_start\n",
      "classifier_end\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "d = model(_forward_S_A_query,_forward_S_A_query_mask,_forward_S_A_query_seg,step=0)\n",
    "print(1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "outputs": [
    {
     "data": {
      "text/plain": "(tensor([[[-0.0560, -0.5592],\n          [-0.2164, -0.1846],\n          [-0.1693,  0.6753],\n          [-0.1545,  0.4091]]], grad_fn=<ViewBackward0>),\n tensor([[[ 0.5829,  0.5612],\n          [ 0.2780,  0.6958],\n          [ 0.2564,  1.2112],\n          [-0.0422,  0.4670]]], grad_fn=<ViewBackward0>),\n tensor([[0.1494, 0.1146, 0.0919, 0.6441],\n         [0.1728, 0.2124, 0.3702, 0.2446],\n         [0.1655, 0.3525, 0.2016, 0.2804],\n         [0.3115, 0.1846, 0.1493, 0.3546]], grad_fn=<SelectBackward0>))"
     },
     "execution_count": 338,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}